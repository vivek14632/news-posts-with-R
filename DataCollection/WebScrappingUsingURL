install.packages('rvest')

library(rvest)
# First, grab the page source
urlcontent<-urls
m<-length(urls$id)
urlcontent$Content <- NA
for(i in 1:m)
{
  print(i)
  tryCatch({
    urlcontent$Content[i] <-  html(urlcontent$link[i]) %>%
  # then extract the first node with class of wikitable
  html_node(".entry-content") %>% 
  # then convert the HTML table into a data frame
  html_text()
  
},
error=function(cond){
  message(paste("No text data for:", urlcontent$id))
  message(cond)
  return(NA)
  }
)
}

setwd("C:/Users/Project/Desktop/Social Media Data/Data")
write.csv(urlcontent, file = "urlcontent.csv")

#remove rows with NA or no contents
Content <- urlcontent$Content
total <- cbind(fb_page_con,Content)
attach(total)
newdata <- subset(total,!is.na(Content))
detach(total) 
setwd("C:/Users/Project/Desktop/Social Media Data/Data")
save(newdata, file="newdata.RData")

#DATA CLEANSING
library(tm)
#Data Cleansing
newdata$Content <- as.character(newdata$Content)
m <- nrow(newdata)
for(n in 1:m)
{
  #remove all punctuations
  newdata$Content[n] <- gsub(pattern = "\\W", replacement = " ",newdata$Content[n])
  
  #print(n)
  #remove all decimals
  newdata$Content[n]<-  gsub(pattern = "\\d", replacement = " ",newdata$Content[n]) 
  
  #convert to lower case
  newdata$Content[n] <- tolower(newdata$Content[n]) 
  
  #remove stop words
  newdata$Content[n] <- removeWords(newdata$Content[n], stopwords()) 
  
  #Remove words with 1  character
  newdata$Content[n] <- gsub(pattern = "\\b[A-z\\b{1}", replace=" ", newdata$Content[n] )
  
  #Remove words with 2  character
  newdata$Content[n] <- gsub(pattern = "\\b[A-z\\b{2}", replace=" ", newdata$Content[n] )
  
}



